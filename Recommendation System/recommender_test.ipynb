{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: transformers in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (4.50.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: faiss-cpu in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: gensim in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (4.3.3)\n",
      "Requirement already satisfied: datasets in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (3.4.1)\n",
      "Requirement already satisfied: networkx in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (3.4.2)\n",
      "Requirement already satisfied: matplotlib in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: ipython in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (9.0.2)\n",
      "Requirement already satisfied: sentencepiece in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: torch in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (8.1.5)\n",
      "Requirement already satisfied: accelerate in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: filelock in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: decorator in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipython) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: psutil in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: wrapt in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from stack_data->ipython) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from stack_data->ipython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/adarshkumardalai/AlmaConnX/.venv/lib/python3.11/site-packages (from stack_data->ipython) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install dependencies (run once)\n",
    "%pip install numpy pandas transformers scikit-learn faiss-cpu gensim datasets networkx matplotlib seaborn tqdm ipython sentencepiece torch torchvision torchaudio ipywidgets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets\n",
    "import torch\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading synthetic dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>About</th>\n",
       "      <th>SkillsText</th>\n",
       "      <th>ExperienceText</th>\n",
       "      <th>EducationText</th>\n",
       "      <th>profile_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Augmented Reality Developer | Creating Immersi...</td>\n",
       "      <td>Dedicated and innovative AR developer with a p...</td>\n",
       "      <td>Unity; ARKit; ARCore; C#; Java; JavaScript; We...</td>\n",
       "      <td>Senior AR Developer | Pixelloid | Zurich, Swit...</td>\n",
       "      <td>ETH Zurich | Bachelor of Science in Computer S...</td>\n",
       "      <td>Headline: Augmented Reality Developer | Creati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UI Designer | Crafting Human-Centered Digital ...</td>\n",
       "      <td>As a UI Designer with a passion for human-cent...</td>\n",
       "      <td>User Experience Design; Visual Design; Human-C...</td>\n",
       "      <td>Senior UI Designer at EVONIK | EVONIK | Essen,...</td>\n",
       "      <td>Hochschule für Gestaltung und Kunst | Bachelor...</td>\n",
       "      <td>Headline: UI Designer | Crafting Human-Centere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Co-Founder AI Product | Transforming Industrie...</td>\n",
       "      <td>Results-driven technology entrepreneur with a ...</td>\n",
       "      <td>AI Strategy; Machine Learning; Deep Learning; ...</td>\n",
       "      <td>Mindera | Co-Founder and Head of AI Product | ...</td>\n",
       "      <td>EPFL | Master of Science in Computer Science |...</td>\n",
       "      <td>Headline: Co-Founder AI Product | Transforming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NLP Specialist | Helping businesses unlock ins...</td>\n",
       "      <td>I'm a seasoned NLP professional with a passion...</td>\n",
       "      <td>NLP; Deep Learning; Natural Language Generatio...</td>\n",
       "      <td>Senior NLP Engineer, IBM Research | IBM | Zuri...</td>\n",
       "      <td>École Polytechnique Fédérale de Lausanne (EPFL...</td>\n",
       "      <td>Headline: NLP Specialist | Helping businesses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Advertising Manager | Driving Brand Growth Thr...</td>\n",
       "      <td>Results-driven Advertising Manager with 8+ yea...</td>\n",
       "      <td>Advertising Strategy; Data Analysis; Team Mana...</td>\n",
       "      <td>Advertising Manager | Bonnard Advertising Agen...</td>\n",
       "      <td>Bachelor's Degree in Marketing | Marketing | U...</td>\n",
       "      <td>Headline: Advertising Manager | Driving Brand ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  \\\n",
       "0  Augmented Reality Developer | Creating Immersi...   \n",
       "1  UI Designer | Crafting Human-Centered Digital ...   \n",
       "2  Co-Founder AI Product | Transforming Industrie...   \n",
       "3  NLP Specialist | Helping businesses unlock ins...   \n",
       "4  Advertising Manager | Driving Brand Growth Thr...   \n",
       "\n",
       "                                               About  \\\n",
       "0  Dedicated and innovative AR developer with a p...   \n",
       "1  As a UI Designer with a passion for human-cent...   \n",
       "2  Results-driven technology entrepreneur with a ...   \n",
       "3  I'm a seasoned NLP professional with a passion...   \n",
       "4  Results-driven Advertising Manager with 8+ yea...   \n",
       "\n",
       "                                          SkillsText  \\\n",
       "0  Unity; ARKit; ARCore; C#; Java; JavaScript; We...   \n",
       "1  User Experience Design; Visual Design; Human-C...   \n",
       "2  AI Strategy; Machine Learning; Deep Learning; ...   \n",
       "3  NLP; Deep Learning; Natural Language Generatio...   \n",
       "4  Advertising Strategy; Data Analysis; Team Mana...   \n",
       "\n",
       "                                      ExperienceText  \\\n",
       "0  Senior AR Developer | Pixelloid | Zurich, Swit...   \n",
       "1  Senior UI Designer at EVONIK | EVONIK | Essen,...   \n",
       "2  Mindera | Co-Founder and Head of AI Product | ...   \n",
       "3  Senior NLP Engineer, IBM Research | IBM | Zuri...   \n",
       "4  Advertising Manager | Bonnard Advertising Agen...   \n",
       "\n",
       "                                       EducationText  \\\n",
       "0  ETH Zurich | Bachelor of Science in Computer S...   \n",
       "1  Hochschule für Gestaltung und Kunst | Bachelor...   \n",
       "2  EPFL | Master of Science in Computer Science |...   \n",
       "3  École Polytechnique Fédérale de Lausanne (EPFL...   \n",
       "4  Bachelor's Degree in Marketing | Marketing | U...   \n",
       "\n",
       "                                        profile_text  \n",
       "0  Headline: Augmented Reality Developer | Creati...  \n",
       "1  Headline: UI Designer | Crafting Human-Centere...  \n",
       "2  Headline: Co-Founder AI Product | Transforming...  \n",
       "3  Headline: NLP Specialist | Helping businesses ...  \n",
       "4  Headline: Advertising Manager | Driving Brand ...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 3: Load and preprocess data\n",
    "print(\"Loading synthetic dataset...\")\n",
    "dataset = load_dataset(\"ilsilfverskiold/linkedin_profiles_synthetic\")\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "df['user_type'] = np.random.choice(['student', 'alumni', 'officer'], len(df))\n",
    "\n",
    "# Rename columns to match metadata and AppConnX conventions\n",
    "df = df.rename(columns={\"About Me\": \"About\", \"Experience\": \"ExperienceText\", \"Education\": \"EducationText\", \"Skills\": \"SkillsText\"})\n",
    "\n",
    "# Combine profile fields into a single text field for summarization\n",
    "def combine_profile(row):\n",
    "    fields = [\n",
    "        f\"Headline: {row['Headline']}\",\n",
    "        f\"About: {row['About']}\",\n",
    "        f\"Skills: {row['SkillsText']}\",\n",
    "        f\"Experience: {row['ExperienceText']}\",\n",
    "        f\"Education: {row['EducationText']}\",\n",
    "        f\"Certifications: {row['Certifications']}\",\n",
    "        f\"Recommendations: {row['Recommendations']}\",\n",
    "        f\"Location: {row['Location']}\"\n",
    "    ]\n",
    "    return \" | \".join([f for f in fields if pd.notna(f) and f.split(': ')[1]])\n",
    "\n",
    "df['profile_text'] = df.apply(combine_profile, axis=1)\n",
    "display(df[['Headline', 'About', 'SkillsText', 'ExperienceText', 'EducationText', 'profile_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec4471af0704c998cc95ef08bebabee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6904 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning T5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2589' max='2589' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2589/2589 2:51:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.005100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.002300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.000900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.001100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.000700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing profiles with fine-tuned T5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a60e079d3a4a3c99f2e908f1f3376a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6904 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile_text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Headline: Augmented Reality Developer | Creati...</td>\n",
       "      <td>Augmented Reality Developer - Skills: About: D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Headline: UI Designer | Crafting Human-Centere...</td>\n",
       "      <td>UI Designer - Skills: About: As a UI Designer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Headline: Co-Founder AI Product | Transforming...</td>\n",
       "      <td>Co-Founder AI Product - Skills: About: Results...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Headline: NLP Specialist | Helping businesses ...</td>\n",
       "      <td>NLP Specialist - Skills: About: I'm a seasoned...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Headline: Advertising Manager | Driving Brand ...</td>\n",
       "      <td>Advertising Manager - Skills: About: Results-d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        profile_text  \\\n",
       "0  Headline: Augmented Reality Developer | Creati...   \n",
       "1  Headline: UI Designer | Crafting Human-Centere...   \n",
       "2  Headline: Co-Founder AI Product | Transforming...   \n",
       "3  Headline: NLP Specialist | Helping businesses ...   \n",
       "4  Headline: Advertising Manager | Driving Brand ...   \n",
       "\n",
       "                                             summary  \n",
       "0  Augmented Reality Developer - Skills: About: D...  \n",
       "1  UI Designer - Skills: About: As a UI Designer ...  \n",
       "2  Co-Founder AI Product - Skills: About: Results...  \n",
       "3  NLP Specialist - Skills: About: I'm a seasoned...  \n",
       "4  Advertising Manager - Skills: About: Results-d...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 4: T5 setup for NLP\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# Prepare fine-tuning dataset\n",
    "def prepare_finetune_data(texts):\n",
    "    input_texts = [f\"summarize: {text}\" for text in texts]\n",
    "    # Use a concise version of profile as target (e.g., Headline + Skills snippet)\n",
    "    target_texts = [f\"{text.split(' | ')[0].replace('Headline: ', '')} - Skills: {text.split(' | ')[2].replace('Skills: ', '').split(', ')[0]}\" \n",
    "                    for text in texts if len(text.split(' | ')) > 2]\n",
    "    return Dataset.from_dict({\"input_text\": input_texts[:len(target_texts)], \"target_text\": target_texts})\n",
    "\n",
    "finetune_dataset = prepare_finetune_data(df['profile_text'].tolist())\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['input_text'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples['target_text'], max_length=100, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = finetune_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Fine-tuning arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    learning_rate=3e-4,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "print(\"Fine-tuning T5...\")\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./t5_finetuned\")\n",
    "tokenizer.save_pretrained(\"./t5_finetuned\")\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"./t5_finetuned\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./t5_finetuned\")\n",
    "\n",
    "def summarize_profile(text):\n",
    "    input_text = f\"summarize: {text}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs['input_ids'], max_length=100, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Summarizing profiles with fine-tuned T5...\")\n",
    "tqdm.pandas()\n",
    "df['summary'] = df['profile_text'].progress_apply(summarize_profile)\n",
    "display(df[['profile_text', 'summary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m embedding_dict = {}\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m embedding_columns:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Extract embeddings as a list\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     embeddings_list = \u001b[43mdf\u001b[49m[col].values\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Convert to numpy array, replacing NaN rows with zeros\u001b[39;00m\n\u001b[32m     20\u001b[39m     embeddings = np.stack([np.nan_to_num(embed, nan=\u001b[32m0.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m embed \u001b[38;5;129;01min\u001b[39;00m embeddings_list])\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 5: Generate embeddings\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# List of precomputed embedding columns from metadata\n",
    "embedding_columns = [\n",
    "    'embeddings_nv-embed-v1', 'embeddings_nv-embedqa-e5-v5', 'embeddings_bge-m3',\n",
    "    'embeddings_arctic-embed-l', 'embeddings_mistral-7b-v2', 'embeddings_gte-large-en-v1.5',\n",
    "    'embeddings_text-embedding-ada-002', 'embeddings_text-embedding-3-small',\n",
    "    'embeddings_voyage-3', 'embeddings_mxbai-embed-large-v1 '\n",
    "]\n",
    "\n",
    "# Normalize precomputed embeddings with NaN handling\n",
    "embedding_dict = {}\n",
    "for col in embedding_columns:\n",
    "    # Extract embeddings as a list\n",
    "    embeddings_list = df[col].values\n",
    "    \n",
    "    # Convert to numpy array, replacing NaN rows with zeros\n",
    "    embeddings = np.stack([np.nan_to_num(embed, nan=0.0) for embed in embeddings_list])\n",
    "    \n",
    "    # Check for rows that were originally all NaN (now all zeros after replacement)\n",
    "    valid_mask = ~np.all(embeddings == 0, axis=1)  # True where row isn’t all zeros\n",
    "    if not np.any(valid_mask):\n",
    "        print(f\"Warning: All embeddings in {col} are invalid (all NaN or zero). Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Filter to valid embeddings only (optional: could keep zeros if desired)\n",
    "    valid_embeddings = embeddings[valid_mask]\n",
    "    print(f\"{col}: {len(valid_embeddings)} valid embeddings out of {len(embeddings)}\")\n",
    "    \n",
    "    # Normalize valid embeddings\n",
    "    embedding_dict[col] = normalize(valid_embeddings)\n",
    "    print(f\"{col} shape: {embedding_dict[col].shape}\")\n",
    "\n",
    "# Example: Use one embedding model for downstream tasks (e.g., bge-m3)\n",
    "selected_embedding = 'embeddings_bge-m3'\n",
    "all_embeddings = embedding_dict.get(selected_embedding, None)\n",
    "if all_embeddings is None:\n",
    "    raise ValueError(f\"Selected embedding {selected_embedding} failed to process.\")\n",
    "print(f\"Using {selected_embedding} with shape: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: FAISS index for similarity search\n",
    "faiss_indices = {}\n",
    "for col in embedding_columns:\n",
    "    d = embedding_dict[col].shape[1]\n",
    "    index = faiss.IndexIVFFlat(faiss.IndexFlatL2(d), d, 50)\n",
    "    index.train(embedding_dict[col])\n",
    "    index.add(embedding_dict[col])\n",
    "    index.nprobe = 5\n",
    "    faiss_indices[col] = index\n",
    "    print(f\"FAISS IVF index for {col} created with {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Graph-based matching\n",
    "# Use one embedding model (e.g., 'embeddings_bge-m3') for graph\n",
    "selected_embedding = 'embeddings_bge-m3'\n",
    "all_embeddings = embedding_dict[selected_embedding]\n",
    "\n",
    "G = nx.Graph()\n",
    "for i, row in df.iterrows():\n",
    "    G.add_node(i, user_type=row['user_type'], embedding=all_embeddings[i])\n",
    "\n",
    "print(\"Building graph edges...\")\n",
    "for i in tqdm(range(len(df))):\n",
    "    for j in range(i + 1, min(i + 50, len(df))):\n",
    "        sim = cosine_similarity([all_embeddings[i]], [all_embeddings[j]])[0][0]\n",
    "        if sim > 0.8:\n",
    "            G.add_edge(i, j, weight=sim)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(G, with_labels=False, node_size=50, node_color=df['user_type'].map({'student': 'blue', 'alumni': 'green', 'officer': 'red'}))\n",
    "plt.title(f\"User Connection Graph ({selected_embedding})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Clustering\n",
    "def cluster_embeddings(embeddings, n_clusters=5):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "    return clusters\n",
    "\n",
    "for col in embedding_columns:\n",
    "    df[f'cluster_{col}'] = cluster_embeddings(embedding_dict[col])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x=embedding_dict[col][:, 0], y=embedding_dict[col][:, 1], hue=df[f'cluster_{col}'], palette='viridis')\n",
    "    plt.title(f\"User Clusters ({col})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Recommendation Systems Comparison\n",
    "def analyze_prompt(prompt):\n",
    "    input_text = f\"extract_keywords: {prompt}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(inputs['input_ids'], max_length=50, num_beams=4, early_stopping=True)\n",
    "    keywords = tokenizer.decode(output_ids[0], skip_special_tokens=True).split(\", \")\n",
    "    prompt_inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        prompt_embedding = model.encoder(**prompt_inputs).last_hidden_state.mean(dim=1).detach().numpy()[0]\n",
    "    return keywords, prompt_embedding\n",
    "\n",
    "def recommend_with_prompt(user_idx, prompt, embedding_col='embeddings_bge-m3', top_n=5):\n",
    "    print(f\"Analyzing prompt: '{prompt}'\")\n",
    "    prompt_keywords, prompt_embedding = analyze_prompt(prompt)\n",
    "    print(f\"Extracted keywords: {prompt_keywords}\")\n",
    "\n",
    "    all_embeddings = embedding_dict[embedding_col]\n",
    "    index = faiss_indices[embedding_col]\n",
    "    user_embedding = all_embeddings[user_idx]\n",
    "    user_type = df.iloc[user_idx]['user_type']\n",
    "    user_skills = df.iloc[user_idx]['SkillsText'].split(', ')\n",
    "\n",
    "    # Adjust prompt embedding dimension if needed (for compatibility)\n",
    "    if prompt_embedding.shape[0] != all_embeddings.shape[1]:\n",
    "        prompt_embedding = generate_t5_embeddings(prompt)[:all_embeddings.shape[1]]\n",
    "\n",
    "    distances, indices = index.search(prompt_embedding.reshape(1, -1), top_n + 10)\n",
    "    candidate_indices = indices[0][indices[0] != user_idx][:top_n + 10]\n",
    "    candidates = df.iloc[candidate_indices]\n",
    "\n",
    "    scores = []\n",
    "    for idx in candidate_indices:\n",
    "        candidate_embedding = all_embeddings[idx]\n",
    "        candidate_type = df.iloc[idx]['user_type']\n",
    "        candidate_skills = df.iloc[idx]['SkillsText'].split(', ')\n",
    "        candidate_experience = df.iloc[idx]['ExperienceText'].lower()\n",
    "\n",
    "        prompt_sim = cosine_similarity([prompt_embedding], [candidate_embedding])[0][0]\n",
    "        user_sim = cosine_similarity([user_embedding], [candidate_embedding])[0][0]\n",
    "\n",
    "        score = 0.7 * prompt_sim + 0.2 * user_sim\n",
    "        if user_type == 'student' and candidate_type == 'alumni':\n",
    "            score += 0.1\n",
    "        skill_overlap = len(set(prompt_keywords) & set(candidate_skills)) / max(len(prompt_keywords), 1)\n",
    "        score += 0.15 * skill_overlap\n",
    "        exp_relevance = sum(1 for kw in prompt_keywords if kw.lower() in candidate_experience) / max(len(prompt_keywords), 1)\n",
    "        score += 0.05 * exp_relevance\n",
    "\n",
    "        scores.append((idx, score, prompt_sim, user_sim, skill_overlap, exp_relevance))\n",
    "\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_indices = [s[0] for s in scores]\n",
    "    top_matches = df.iloc[top_indices][['FirstName', 'LastName', 'user_type', 'Headline', 'SkillsText']]\n",
    "\n",
    "    explanations = [\n",
    "        f\"Match {i+1}: Score={s[1]:.3f}, Prompt Sim={s[2]:.3f}, User Sim={s[3]:.3f}, Skill Overlap={s[4]:.3f}, Exp Relevance={s[5]:.3f}\"\n",
    "        for i, s in enumerate(scores)\n",
    "    ]\n",
    "    return top_matches, explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Evaluation and Comparison (Fixed)\n",
    "\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "def test_prompt(user_idx, prompt, embedding_model):\n",
    "    user_name = f\"{df.iloc[user_idx]['FirstName']} {df.iloc[user_idx]['LastName']}\"\n",
    "    print(f\"User: {user_name} (Type: {df.iloc[user_idx]['user_type']})\")\n",
    "    matches, explanations = recommend_with_prompt(user_idx, prompt, embedding_model)\n",
    "    display(HTML(f\"<b>Top Matches for '{prompt}' using {embedding_model}:</b>\"))\n",
    "    display(matches)\n",
    "    display(HTML(\"<b>Explanations:</b>\"))\n",
    "    for exp in explanations:\n",
    "        print(exp)\n",
    "\n",
    "user_slider = widgets.IntSlider(min=0, max=len(df)-1, step=1, value=100, description='User Index')\n",
    "prompt_text = widgets.Text(value=\"Frontend developer with React experience\", description='Prompt')\n",
    "embedding_dropdown = widgets.Dropdown(options=embedding_columns, value='embeddings_bge-m3', description='Embedding Model')\n",
    "interact(test_prompt, user_idx=user_slider, prompt=prompt, embedding_model=embedding_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualize match quality\n",
    "def plot_match_quality(user_idx, prompt, embedding_model):\n",
    "    matches, explanations = recommend_with_prompt(user_idx, prompt, embedding_model)\n",
    "    scores = [float(exp.split(\"Score=\")[1].split(\",\")[0]) for exp in explanations]\n",
    "    labels = [f\"Match {i+1}\" for i in range(len(scores))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=labels, y=scores, palette='viridis')\n",
    "    plt.title(f\"Match Quality for '{prompt}' ({embedding_model})\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1.5)\n",
    "    plt.show()\n",
    "\n",
    "plot_match_quality(100, \"Android developer with Kotlin experience\", 'embeddings_bge-m3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cell 12: Comparison of Embedding Models\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def compare_embeddings(prompt, user_idx=100, top_n=5):\n",
    "    results = {}\n",
    "    for col in embedding_columns:\n",
    "        matches, explanations = recommend_with_prompt(user_idx, prompt, col, top_n)\n",
    "        scores = [float(exp.split(\"Score=\")[1].split(\",\")[0]) for exp in explanations]\n",
    "        \n",
    "        # Cosine similarity consistency (average similarity to prompt embedding)\n",
    "        prompt_embedding = generate_t5_embeddings(prompt)  # T5 as reference\n",
    "        top_embeddings = embedding_dict[col][matches.index]\n",
    "        avg_prompt_sim = np.mean([cosine_similarity([prompt_embedding[:top_embeddings.shape[1]]], [e])[0][0] for e in top_embeddings])\n",
    "        \n",
    "        # Clustering quality (silhouette score)\n",
    "        cluster_labels = df[f'cluster_{col}'].iloc[matches.index]\n",
    "        if len(set(cluster_labels)) > 1:  # Need at least 2 clusters\n",
    "            sil_score = silhouette_score(top_embeddings, cluster_labels)\n",
    "        else:\n",
    "            sil_score = 0\n",
    "        \n",
    "        results[col] = {\n",
    "            'avg_score': np.mean(scores),\n",
    "            'avg_prompt_sim': avg_prompt_sim,\n",
    "            'silhouette_score': sil_score\n",
    "        }\n",
    "        print(f\"{col}: Avg Score={results[col]['avg_score']:.3f}, Prompt Sim={results[col]['avg_prompt_sim']:.3f}, Silhouette={results[col]['silhouette_score']:.3f}\")\n",
    "\n",
    "    # Plot comparison\n",
    "    metrics = ['avg_score', 'avg_prompt_sim', 'silhouette_score']\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=list(results.keys()), y=[r[metric] for r in results.values()], palette='viridis')\n",
    "        plt.title(f\"Embedding Model Comparison: {metric}\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel(metric)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Test comparison\n",
    "compare_embeddings(\"Data scientist with Python experience\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
