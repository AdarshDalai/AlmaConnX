{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.conda (Python 3.11.11)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -p /Users/adarshkumardalai/Project AlmaConnX/.conda ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install dependencies (run once)\n",
    "%pip install numpy pandas transformers scikit-learn faiss-cpu gensim datasets networkx matplotlib seaborn tqdm ipython sentencepiece torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import faiss\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load and preprocess data\n",
    "print(\"Loading synthetic dataset...\")\n",
    "dataset = load_dataset(\"ilsilfverskiold/linkedin_profiles_synthetic\")\n",
    "df = pd.DataFrame(dataset['train'].select(range(500))) # Limit to 500 rows for speed\n",
    "df['user_type'] = np.random.choice(['student', 'alumni', 'officer'], len(df))\n",
    "display(df.head()) # Display sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: T5 setup for NLP\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "def summarize_profile(text):\n",
    "    try:\n",
    "        input_text = f\"summarize: {text}\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        summary_ids = model.generate(inputs['input_ids'], max_length=50, num_beams=4, early_stopping=True)\n",
    "        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error summarizing: {e}\")\n",
    "        return text\n",
    "\n",
    "print(\"Summarizing profiles...\")\n",
    "tqdm.pandas() # Enable progress bar for pandas\n",
    "df['summary'] = df['Headline'].progress_apply(summarize_profile)\n",
    "display(df[['Headline', 'summary']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Generate embeddings\n",
    "def generate_t5_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model.encoder(**inputs).last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return outputs[0]\n",
    "\n",
    "print(\"Generating T5 embeddings...\")\n",
    "df['headline_embedding'] = [generate_t5_embeddings(text) for text in tqdm(df['Headline'])]\n",
    "df['summary_embedding'] = [generate_t5_embeddings(text) for text in tqdm(df['summary'])]\n",
    "\n",
    "# Combine embeddings\n",
    "df['combined_embedding'] = df.apply(lambda row: np.mean([row['headline_embedding'], row['summary_embedding']], axis=0), axis=1)\n",
    "all_embeddings = normalize(np.stack(df['combined_embedding'].values))\n",
    "print(f\"Embeddings shape: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: FAISS index for similarity search\n",
    "index = faiss.IndexFlatL2(all_embeddings.shape[1])\n",
    "index.add(all_embeddings)\n",
    "print(\"FAISS index created with\", index.ntotal, \"vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Graph-based matching\n",
    "G = nx.Graph()\n",
    "for i, row in df.iterrows():\n",
    "    G.add_node(i, user_type=row['user_type'], embedding=row['combined_embedding'])\n",
    "\n",
    "print(\"Building graph edges...\")\n",
    "for i in tqdm(range(len(df))):\n",
    "    for j in range(i + 1, min(i + 50, len(df))): # Limit comparisons for speed\n",
    "        sim = cosine_similarity([all_embeddings[i]], [all_embeddings[j]])[0][0]\n",
    "        if sim > 0.8:\n",
    "            G.add_edge(i, j, weight=sim)\n",
    "\n",
    "# Visualize graph (small subset)\n",
    "plt.figure(figsize=(8, 6))\n",
    "nx.draw(G, with_labels=False, node_size=50, node_color=df['user_type'].map({'student': 'blue', 'alumni': 'green', 'officer': 'red'}))\n",
    "plt.title(\"User Connection Graph\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(all_embeddings)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=all_embeddings[:, 0], y=all_embeddings[:, 1], hue=df['cluster'], palette='viridis')\n",
    "plt.title(\"User Clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Enhanced recommendation function with prompt analysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Function to analyze prompt and extract requirements\n",
    "def analyze_prompt(prompt):\n",
    "    input_text = f\"extract_keywords: {prompt}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    output_ids = model.generate(inputs['input_ids'], max_length=50, num_beams=4, early_stopping=True)\n",
    "    keywords = tokenizer.decode(output_ids[0], skip_special_tokens=True).split(\", \")\n",
    "    \n",
    "    # Generate prompt embedding\n",
    "    prompt_embedding = generate_t5_embeddings(prompt)\n",
    "    return keywords, prompt_embedding\n",
    "\n",
    "# Enhanced recommendation function with prompt\n",
    "def recommend_with_prompt(user_idx, prompt, top_n=5):\n",
    "    # Step 1: Analyze prompt\n",
    "    print(f\"Analyzing prompt: '{prompt}'\")\n",
    "    prompt_keywords, prompt_embedding = analyze_prompt(prompt)\n",
    "    print(f\"Extracted keywords: {prompt_keywords}\")\n",
    "\n",
    "    # Step 2: Get user profile\n",
    "    user_embedding = all_embeddings[user_idx]\n",
    "    user_type = df.iloc[user_idx]['user_type']\n",
    "    user_headline = df.iloc[user_idx]['Headline']\n",
    "\n",
    "    # Step 3: Search FAISS for content-based matches\n",
    "    distances, indices = index.search(prompt_embedding.reshape(1, -1), top_n + 10) # Extra candidates\n",
    "    candidate_indices = indices[0][1:] # Exclude prompt itself if indexed\n",
    "    candidates = df.iloc[candidate_indices]\n",
    "\n",
    "    # Step 4: Score candidates based on prompt and user profile\n",
    "    scores = []\n",
    "    for idx in candidate_indices:\n",
    "        candidate_embedding = all_embeddings[idx]\n",
    "        candidate_headline = df.iloc[idx]['Headline']\n",
    "        candidate_type = df.iloc[idx]['user_type']\n",
    "\n",
    "        # Similarity scores\n",
    "        prompt_sim = cosine_similarity([prompt_embedding], [candidate_embedding])[0][0]\n",
    "        user_sim = cosine_similarity([user_embedding], [candidate_embedding])[0][0]\n",
    "\n",
    "        # Weighted scoring (adjust weights as needed)\n",
    "        score = 0.6 * prompt_sim + 0.4 * user_sim\n",
    "        \n",
    "        # Bonus for user type alignment (e.g., student -> alumni)\n",
    "        if user_type == 'student' and candidate_type == 'alumni':\n",
    "            score += 0.1\n",
    "        \n",
    "        # Bonus for keyword overlap\n",
    "        headline_keywords = candidate_headline.lower().split()\n",
    "        keyword_overlap = len(set(prompt_keywords) & set(headline_keywords)) / len(prompt_keywords)\n",
    "        score += 0.2 * keyword_overlap\n",
    "\n",
    "        scores.append((idx, score, prompt_sim, user_sim, keyword_overlap))\n",
    "\n",
    "    # Step 5: Rank and filter top matches\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_indices = [s[0] for s in scores]\n",
    "    top_matches = df.iloc[top_indices][['FirstName', 'LastName', 'user_type', 'Headline']]\n",
    "    \n",
    "    # Step 6: Prepare explanations\n",
    "    explanations = []\n",
    "    for i, (idx, score, prompt_sim, user_sim, keyword_overlap) in enumerate(scores):\n",
    "        explanation = (\n",
    "            f\"Match {i+1}: Score={score:.3f}, \"\n",
    "            f\"Prompt Similarity={prompt_sim:.3f}, \"\n",
    "            f\"User Similarity={user_sim:.3f}, \"\n",
    "            f\"Keyword Overlap={keyword_overlap:.3f}\"\n",
    "        )\n",
    "        explanations.append(explanation)\n",
    "    \n",
    "    return top_matches, explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Interactive prompt input and testing\n",
    "from ipywidgets import interact, widgets\n",
    "\n",
    "# Define a function for interactive testing\n",
    "def test_prompt(user_idx, prompt):\n",
    "    user_name = f\"{df.iloc[user_idx]['FirstName']} {df.iloc[user_idx]['LastName']}\"\n",
    "    print(f\"User: {user_name} (Type: {df.iloc[user_idx]['user_type']})\")\n",
    "    matches, explanations = recommend_with_prompt(user_idx, prompt)\n",
    "    \n",
    "    display(HTML(f\"<b>Top Matches for '{prompt}':</b>\"))\n",
    "    display(matches)\n",
    "    display(HTML(\"<b>Explanations:</b>\"))\n",
    "    for exp in explanations:\n",
    "        print(exp)\n",
    "\n",
    "# Create interactive widget\n",
    "user_slider = widgets.IntSlider(min=0, max=len(df)-1, step=1, value=100, description='User Index')\n",
    "prompt_text = widgets.Text(value=\"Any Android Developer to connect with?\", description='Prompt')\n",
    "interact(test_prompt, user_idx=user_slider, prompt=prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Visualize match quality\n",
    "def plot_match_quality(user_idx, prompt):\n",
    "    matches, explanations = recommend_with_prompt(user_idx, prompt)\n",
    "    scores = [float(exp.split(\"Score=\")[1].split(\",\")[0]) for exp in explanations]\n",
    "    labels = [f\"Match {i+1}\" for i in range(len(scores))]\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=labels, y=scores, palette='viridis')\n",
    "    plt.title(f\"Match Quality for '{prompt}'\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0, 1.5) # Adjust based on score range\n",
    "    plt.show()\n",
    "\n",
    "# Test visualization\n",
    "plot_match_quality(100, \"Any Android Developer to connect with?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
